\section{Related Work}
\label{relWork}
On the architecture side, buffer management strategies to reduce PCM latency and energy consumption are discussed in \cite{lee}. Wear levelling algorithm are proposed in \cite{wear} that rotate the lines within a circular buffer each time a certain write threshold is reached. A randomized algorithm was introduced to handle the case when the writes are spatially concentrated to enable wear levelling across entire PCM. Techniques to reduce writes by writing back only modified data to PCM upon eviction from LLC/DRAM are presented in \cite{qureshi, write, lee, zhou}. In Flip-N-Write scheme \cite{flipnwrite}, a modified data word or its complement is stored depending on whose Hamming distance to the original word is lesser. As a result, it restricts the maximum bit writes per word to $B/2$, where \textit{B} is the number of bits in a word.

On the database algorithms front, \cite{cost_aware} seeks to build the PCM read-write asymmetry information within the query optimizer itself. This is orthogonal to our work since we try to optimise for writes during the query execution stage. Writes reduction for B$^+$ Tree index and hash join for \modelPcmRam{} model are proposed in \cite{chen}. It recommends keeping the keys unsorted at the leaf nodes of the index. This would incur extra search time in the leaf nodes but save writes. For partitioning during hash join, a pointer based partitioning approach is proposed to avoid full tuple writes. Since we assume database to be PCM-resident, the partitioning step is obviated in our algorithms. 

Sort and join algorithms for system model \modelExplicit{} are presented in \cite{viglas}. Two classes of algorithms have been proposed for both sort and join. The first class of algorithms divides the input into write incurring and write limited parts. The write incurring part finishes in a single pass whereas the write limited part finishes in  multiple iterations. In the second class of algorithms, the materialization of intermediate results is deferred until the read cost (in terms of time) exceeds the write cost. Our work differs from this work since, unlike their model, our model has no explicit control over DRAM. This means that we cannot selectively decide what to keep in DRAM at any point of time. It also implies that we may ultimately end up getting much lesser DRAM space than we had anticipated, due to other programs running in parallel on the system. As shown in Section \ref{sec:exp}, our algorithms have been designed in such a way that even in the worst case availability, we would do better than conventional algorithms in terms of writes. However, if we consider the sorting algorithms proposed there such as the \textit{lazy sort} algorithm, it uses a heap that may keep constantly getting updated in each pass. If the available DRAM happens to be lesser than the heap size, it is likely that the updated entries keep getting evicted causing a large number of writes. Secondly, the join algorithms proposed there involve partitioning the data for the hash table to fit in DRAM. However, since the results are written out simultaneously with the join process, and given the output tuples count for join can be multiplicative (i.e. if $M$ and $N$ is the cardinality of the input relations, there can potentially be $MN$ output tuples), it is likely that, unless the partitions are very small, the hash table gets evicted even after partitioning. 

Sorting algorithms for \modelExplicit{} model are discussed in \cite{vamsi}. They split the given range into buckets such that each bucket can be sorted using DRAM. The bucket boundaries are determined using hybrid histograms having both depth bound and width bound buckets, the bound being decided depending upon which limit is hit \textit{later}. The elements are then shuffled to group same bucket elements together followed by sorting each bucket using DRAM. The sorting methodology used is quicksort or count sort based on whether the bucket is depth bound or width bound respectively. A major drawback with this approach is that there is a large likelihood of an error in the approximation of the histogram, leading to DRAM overflow in some of the buckets. This would lead to additional writes since the buckets then need to be split. Besides, the construction of a histogram itself incurs writes of its own.

\begin{comment}
Table \ref{tab:prev_work} summarises the previous work on PCM-conscious database operators algorithms.
\end{comment}
There has been related research to speed up query execution for \textit{flash} resident databases. The use of a column based layout has been advocated in \cite{graefe} to avoid fetching unnecessary attributes during scan. The same layout is also leveraged for joins by fetching only the columns participating in the join, deferring full tuple materialization to as late as possible in the plan tree. External Merge sort is recommended for data not fitting in DRAM. These techniques, though applicable to a PCM setting, are orthogonal to our work.